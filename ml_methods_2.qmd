---
title: "ML Methods"
subtitle: "Comprehensive Data Analysis of AI and Non-AI Jobs Using K-Means Clustering, Regression Analysis, and Random Forest"
format:
    html:
        toc: true
        number-sections: true
        df-print: paged
execute:
    echo: false
---


# Unsupervised Clustering


As a group, we will be implementing an unsupervised clustering model using the K-mean algorithm. We are seeking to differentiate AI-related jobs postings from non-AI ones. AI-related job postings will be defined by AI-related keywords. The results will serve to tell us how how certain features separate AI-related job postings from non-AI.


```{python echo=false}
#from pyspark.ml.feature import VectorAssembler
#from pyspark.ml.feature import StringIndexer
#from pyspark.sql.functions import col, lower, when
#from pyspark.sql.types import DoubleType
#from pyspark.ml.clustering import KMeans
#from pyspark.ml import Pipeline
#from pyspark.ml.evaluation import ClusteringEvaluator
#from pyspark.sql import SparkSession
# Load the Lightcast Job Posting Data
#spark = SparkSession.builder \
#.appName("AI_vs_NonAI_JobPostings") \
#.config("spark.driver.memory", "4g") \
#.getOrCreate()




#df = spark.read.option("header", "true") \
#    .option("inferSchema", "true") \
#    .option("multiLine", "true") \
#    .option("escape", "\"") \
#    .csv("./data/lightcast_job_postings.csv")
```

```{python echo=false}
# Drop unnecessary columns
#columns_to_drop = [
#"ID", "URL", "ACTIVE_URLS", "DUPLICATES", #"LAST_UPDATED_TIMESTAMP",
#"NAICS2", "NAICS3", "NAICS4", "NAICS5", "NAICS6",
#"SOC_2", "SOC_3", "SOC_5"
#]
#df = df.drop(*columns_to_drop)


# Filter rows with non-null salary and classification columns
#df = df.filter(
#(col("SALARY_FROM").isNotNull()) &
#(col("SALARY_TO").isNotNull()) &
#(col("TITLE_RAW").isNotNull()) &
#(col("NAICS_2022_6").isNotNull()) &
#(col("MIN_YEARS_EXPERIENCE").isNotNull()) &
#(col("MAX_YEARS_EXPERIENCE").isNotNull())
#)


# Convert salary columns to numeric
#df = df.withColumn("SALARY_FROM", col("SALARY_FROM").cast(DoubleType()))
#df = df.withColumn("SALARY_TO", col("SALARY_TO").cast(DoubleType#()))


# Create average salary column
#df = df.withColumn("AVG_SALARY", (col("SALARY_FROM") + col("SALARY_TO")) / 2)


# Convert experience columns and create average experience column
#df = df.withColumn("MIN_YEARS_EXPERIENCE", col("MIN_YEARS_EXPERIENCE").cast(DoubleType()))
#df = df.withColumn("MAX_YEARS_EXPERIENCE", col("MAX_YEARS_EXPERIENCE").cast(DoubleType()))
#df = df.withColumn("AVG_YEARS_EXPERIENCE", (col("MIN_YEARS_EXPERIENCE") + col("MAX_YEARS_EXPERIENCE")) / 2)


# Remove rows with null values in critical columns
#df = df.dropna(subset=['AVG_SALARY', 'TITLE_RAW', 'NAICS_2022_6', 'AVG_YEARS_EXPERIENCE'])


# Lowercase the TITLE_RAW column for keyword matching
#df = df.withColumn("TITLE_LOWER", lower(col("TITLE_RAW")))


# List of AI keywords
#ai_keywords = [
#"ai", "artificial intelligence", "machine learning", "deep learning",
#"neural network", "nlp", "computer vision", "chatgpt", "gpt-3", "gpt-4",
#"llm", "large language model"
#]


# Build condition for presence of AI keywords
#ai_condition = None
#for keyword in ai_keywords:
#    condition = col("TITLE_LOWER").contains(keyword)
#    if ai_condition is None:
#        ai_condition = condition
#    else:
#        ai_condition = ai_condition | condition


# Create binary column IS_AI_ROLE
#df = df.withColumn("IS_AI_ROLE", when(ai_condition, 1).otherwise(0))
```
```{python echo=false}
#from pyspark.ml.feature import StringIndexer
#from pyspark.ml.feature import VectorAssembler
#from pyspark.ml.clustering import KMeans
#from pyspark.ml import Pipeline


# Define pipeline stages
#title_indexer = StringIndexer(inputCol="TITLE_RAW", outputCol="TITLE_ENCODED", handleInvalid="keep")
#naics_indexer = StringIndexer(inputCol="NAICS_2022_6", outputCol="NAICS_ENCODED", handleInvalid="keep")
#assembler = VectorAssembler(
#inputCols=["TITLE_ENCODED", "NAICS_ENCODED", "AVG_SALARY", "AVG_YEARS_EXPERIENCE"],
#outputCol="features"
#)
#kmeans = KMeans(featuresCol="features", predictionCol="CLUSTER", k=2, seed=42)


# Create pipeline
#pipeline = Pipeline(stages=[title_indexer, naics_indexer, #assembler, kmeans])


# Fit pipeline
#model = pipeline.fit(df)
```


# Features and Silhouette Score


We will be using the following features for our clustering model:


- Average Salary, expected to capture pay differences
- Job Title (Encoded), expected to capture role differences
- NAICS Code (Encoded), expected to separate jobs by sectors
- Average Years of Experience, expected to capture experience level differences


Grouping these features in a cluster of 2, we found a silhouette score of approximately 0.695, indicating a good clustering structure. However, since the score is not closer to 1 than expected, reflecting the complexity between AI and non-AI job postings.


| IS_AI_ROLE | CLUSTER | count |
|------------|---------|-------|
| 1          | 0       | 47    |
| 1          | 1       | 19    |
| 0          | 0       | 2195  |
| 0          | 1       | 1669  |

The table shows the count of jobs by AI role and cluster. There are 66 AI - Labeled jobs total. Non - AI jobs are more numerous, with 3,864 present in the data set.




```{python echo=false}
#from pyspark.ml.evaluation import ClusteringEvaluator
# Transform data to get cluster predictions
#predictions = model.transform(df)


# Evaluate clustering
#evaluator = ClusteringEvaluator(featuresCol="features", predictionCol="CLUSTER", metricName="silhouette")
#silhouette = evaluator.evaluate(predictions)
#print(f"Silhouette score = {silhouette}")


# Show cluster counts grouped by AI role
#predictions.groupBy("IS_AI_ROLE", "CLUSTER").count().show()
```


# Pairwise Plots


Below are a series of pairwise plots showing the relationships between the features used in the clustering model, consisting of Average Salary, Job Title (Encoded), NAICS Code (Encoded), and Average Years of Experience. The plots seek to visualize how these features interact and potentially separate AI-related job postings from non-AI ones. From the plots, a clear relationship is not visible between the features due to the lower number of AI-related job postings in the dataset.


It would be unusual to see any AI jobs since AI jobs are not common in the hiring process. The few present in the visualizations are likely the ones that require higher technical skills in the ML domain. To further study our prompt, other processes will need to be performed.






```{python echo=false}


#import hvplot.pandas
#import panel as pn
#pn.extension()


# Sample 5% of the data to reduce size for plotting
#sample_pdf = predictions.select(
#"AVG_SALARY", "TITLE_ENCODED", "NAICS_ENCODED", #"AVG_YEARS_EXPERIENCE", "IS_AI_ROLE"
#).sample(fraction=0.05, seed=42).toPandas()


# Map IS_AI_ROLE to string labels for coloring
#sample_pdf['Role'] = sample_pdf['IS_AI_ROLE'].map({0: 'Non-AI', 1: 'AI'})


# Columns to plot
#cols = ["AVG_SALARY", "TITLE_ENCODED", "NAICS_ENCODED", "AVG_YEARS_EXPERIENCE"]


#plots = []
#for x in cols:
#    for y in cols:
#        if x != y:
#            plot = sample_pdf.hvplot.scatter(
#                x=x, y=y, by='Role', width=450, height=250, alpha=0.6, title=f"{y} vs {x}"
#            )
#            plots.append(plot)


# Arrange plots in a grid with 2 columns
#grid = pn.GridBox(*plots, ncols=2)


# Save the interactive grid as an HTML file
#grid.save("figures/interactive_plots.html", embed=True)
```


[View Interactive Plots](figures/interactive_plots.html)


# PCA 2D Visualization Excluding Salary


Generating a 2D PCA plot excluding salary to visualize the clustering of AI-related and non-AI jobs based served as a useful alternative to the previous pairwise plots. The following PCA plots consist of PC1 vs PC2, PC1 vs PC3, and PC2 vs PC3.


The PC1 vs PC2 plots shows some separation between clusters along PC1, but a lot of overlap along PC2. The PC1 vs PC3 plot demonstrates a clearer separation between the clusters, with cluster groups more distinctly divided along PC3. The PC2 vs PC3 presents moderate cluster separation, with the appearance of vertical banding along PC2.


The results of the PCA plots indicate the there are now two visually distinct clusters, which is an improvement from the previous pairwise plots. However, there is still confusion regarding which features strongly divide AI-related job postings from non-AI ones.




```{python echo=false}


#from pyspark.ml.feature import VectorAssembler, PCA
#import seaborn as sns
#import matplotlib.pyplot as plt


# Step 1: Assemble features excluding AVG_SALARY
#assembler_no_salary = VectorAssembler(
#inputCols=["TITLE_ENCODED", "NAICS_ENCODED", "AVG_YEARS_EXPERIENCE"],
#outputCol="features_no_salary"
#)


#df_features = assembler_no_salary.transform(predictions)


# Step 2: Fit PCA with k=3 (max components for 3 features)
#pca = PCA(k=3, inputCol="features_no_salary", outputCol="pcaFeatures")
#pca_model = pca.fit(df_features)
#pca_result = pca_model.transform(df_features)


# Step 3: Convert to Pandas for plotting
#pdf_pca = pca_result.select("pcaFeatures", "CLUSTER").toPandas()


# Extract PCA components to separate columns
#pdf_pca["PC1"] = pdf_pca["pcaFeatures"].apply(lambda x: x[0])
#pdf_pca["PC2"] = pdf_pca["pcaFeatures"].apply(lambda x: x[1])
#pdf_pca["PC3"] = pdf_pca["pcaFeatures"].apply(lambda x: x[2])


# Step 4: Plotting and saving function
#def plot_pcs_save(x, y, filename):
#    sns.scatterplot(data=pdf_pca, x=x, y=y, hue="CLUSTER", #palette="deep", alpha=0.6)
#    plt.title(f"PCA Scatter Plot: {x} vs {y}")
#    plt.savefig(f"figures/{filename}")
#    plt.close()


# Save PCA scatter plots as PNG images
#plot_pcs_save("PC1", "PC2", "pca_PC1_vs_PC2.png")
#plot_pcs_save("PC1", "PC3", "pca_PC1_vs_PC3.png")
#plot_pcs_save("PC2", "PC3", "pca_PC2_vs_PC3.png")
```


![](figures/pca_PC1_vs_PC2.png)


![](figures/pca_PC1_vs_PC3.png)


![](figures/pca_PC2_vs_PC3.png)




```{python echo=false}


# pca loadings from the data, and it will show which variable contributes to which component
#from pyspark.ml.feature import VectorAssembler, PCA
#import pandas as pd


# Replace with your actual feature columns
#features_list = ["TITLE_ENCODED", "NAICS_ENCODED", "AVG_YEARS_EXPERIENCE"]


# Step 1: Assemble features into a vector column
#assembler = VectorAssembler(inputCols=features_list, outputCol="features_no_salary")
#df_features = assembler.transform(predictions)


# Step 2: Fit PCA with k=3 components
#no_of_components = 3
#pca = PCA(k=no_of_components, inputCol="features_no_salary", outputCol="pcaFeatures")
#pca_model = pca.fit(df_features)
```
```{python echo=false}


# Step 3: Transform data to get PCA features (optional, for inspection)
#pca_result = pca_model.transform(df_features).select("pcaFeatures")
#pca_result.show(truncate=False)
```




# PCA Loadings Table


| Variable             | PCA1       | PCA2       | PCA3       |
|----------------------|------------|------------|------------|
| TITLE_ENCODED        | -0.999826  | 0.018638   | -0.000502  |
| NAICS_ENCODED        | -0.018640  | -0.999809  | 0.005905   |
| AVG_YEARS_EXPERIENCE | -0.000392  | 0.005913   | 0.999982   |


Loading the data set, PC1 is entirely dominated by TITLE_ENCODED, with a coefficient of 0.999. This means the differences in job titles are the main factor separating the clusters. The second largest variance, PC2, is dominated by NAICS_ENCODED with a coefficient of 0.998, indicating that the industry sector is the next most important factor. PC3 is dominated by AVG_YEARS_EXPERIENCE with a coefficient of 0.999, suggesting that experience level is also a significant factor in distinguishing between job postings, which could lead to differences in salary.


# Differences Between AI and Non-AI Job Postings Using K-Means Clustering


Using K-means clustering, we identified two clusters in our dataset through a series of processes including selecting features, calculating a silhouette score, and visualizing the data using PCA 2D plots. Although this is an unsupervised learning method, the clusters likely correspond to AI-related and non-AI jobs postings based on the features used. Clusters are primarily separated by job titles, a strong indicator of whether a job is AI-related. Industry classification and experience level also play a role in distinguishing between the two types of job postings. The process, while not definitive, provides insights into the characteristics that differentiate AI-related jobs postings from non-AI ones in the dataset.

# A Table Displaying AI Related Jobs

```

+--------------------------------------------------------------------------+------------+----------+--------------------+
|TITLE_RAW                                                                 |NAICS_2022_6|AVG_SALARY|AVG_YEARS_EXPERIENCE|
+--------------------------------------------------------------------------+------------+----------+--------------------+
|Data / AI / ML Analyst - Full time / Onsite Venice, FL                    |424210      |77500.0   |2.0                 |
|Senior AI Engineer                                                        |334111      |183250.0  |5.0                 |
|Data Analyst - BI & AI                                                    |237310      |89500.0   |2.0                 |
|Platform Architect - Analytics AI/ML (Remote)                             |221118      |168150.0  |8.0                 |
|Government and Public Sector - AI & Data Analyst (TS/SCI Clearan NEW      |541211      |99850.0   |1.0                 |
|Platform Architect - Analytics AI/ML (Remote)                             |221118      |168150.0  |8.0                 |
|Data Analyst - Generative AI Specialist                                   |541512      |95000.0   |1.0                 |
|Enterprise Ai/Ml Architect                                                |561311      |107500.0  |3.0                 |
|Enterprise AI/ML Architect                                                |517121      |107500.0  |3.0                 |
|Platform Architect - Analytics AI/ML (Remote)                             |221118      |168150.0  |8.0                 |
|(USA) Senior, Data Analyst - Data Science and AI f...                     |455211      |117500.0  |3.0                 |
|(USA) Senior, Data Analyst - Data Science and AI for Corporate Real Estate|455211      |117500.0  |3.0                 |
|Enterprise AI/ML Architect                                                |517121      |107500.0  |3.0                 |
|Data Analyst - Generative AI Specialist                                   |541512      |115000.0  |1.0                 |
|Enterprise AI/ML Architect                                                |517121      |107500.0  |3.0                 |
|Data Analyst - BI & AI                                                    |237310      |89500.0   |2.0                 |
|Data Analyst - BI & AI                                                    |237310      |89500.0   |2.0                 |
|SAP ABAP Consultant with GEN AI                                           |541511      |122501.5  |8.0                 |
+--------------------------------------------------------------------------+------------+----------+--------------------+


```
The above table presents a case where experienced, or in other words, highly skilled jobs meet the criteria for being deemed AI jobs. This table supports the idea that experienced positions are closely associated with AI, considering their complexity.


```{python echo=false}


# Step 4: Extract PCA loadings matrix
#loadings_matrix = pca_model.pc.toArray()


# Step 5: Create Pandas DataFrame for loadings
#loading_scores = pd.DataFrame(loadings_matrix, columns=[f"PCA{i+1}" for i in range(no_of_components)])
#loading_scores["Variable"] = features_list


# Reorder columns for readability
#loading_scores = loading_scores[["Variable"] + [f"PCA{i+1}" for i in range(no_of_components)]]


# Step 6: Print loadings
#print(loading_scores)
```

```{python echo=false}
# Filter the DataFrame to get only AI jobs
# Filter the DataFrame to get only AI jobs
#ai_jobs_df = df.filter(col("IS_AI_ROLE") == 1)
#ai_jobs_df.select("TITLE_RAW", "NAICS_2022_6", "AVG_SALARY", "AVG_YEARS_EXPERIENCE").show(truncate=False)
```




```{python echo=false}
# from pyspark.sql import SparkSession
# from pyspark.sql.functions import col, when, lower
# from pyspark.sql.types import DoubleType
# from pyspark.ml import Pipeline
# from pyspark.ml.feature import StringIndexer, VectorAssembler
# from pyspark.ml.classification import LogisticRegression
# from pyspark.ml.evaluation import BinaryClassificationEvaluator

# # initialize spark session
# spark = SparkSession.builder.appName("ai_job_classification_weighted").config("spark.driver.memory", "4g").getOrCreate()

# # load dataset
# df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine", "true").option("escape", "\"").csv("./lightcast_job_postings.csv")

# # drop unnecessary columns
# columns_to_drop = ["ID", "URL", "ACTIVE_URLS", "DUPLICATES", "LAST_UPDATED_TIMESTAMP", "NAICS2", "NAICS3", "NAICS4", "NAICS5", "NAICS6", "SOC_2", "SOC_3", "SOC_5"]
# df = df.drop(*columns_to_drop)

# # filter for non-null critical columns
# df = df.filter((col("TITLE_RAW").isNotNull()) & (col("NAICS_2022_6").isNotNull()) & (col("MIN_YEARS_EXPERIENCE").isNotNull()) & (col("MAX_YEARS_EXPERIENCE").isNotNull()))

# # convert experience columns and create average
# df = df.withColumn("MIN_YEARS_EXPERIENCE", col("MIN_YEARS_EXPERIENCE").cast(DoubleType()))
# df = df.withColumn("MAX_YEARS_EXPERIENCE", col("MAX_YEARS_EXPERIENCE").cast(DoubleType()))
# df = df.withColumn("AVG_YEARS_EXPERIENCE", (col("MIN_YEARS_EXPERIENCE") + col("MAX_YEARS_EXPERIENCE")) / 2)

# # create ai role indicator
# ai_keywords = ["ai", "artificial intelligence", "machine learning", "deep learning", "neural network", "nlp", "computer vision", "chatgpt", "gpt-3", "gpt-4", "llm", "large language model"]
# df = df.withColumn("TITLE_LOWER", lower(col("TITLE_RAW")))
# ai_condition = None
# for keyword in ai_keywords:
#     condition = col("TITLE_LOWER").contains(keyword)
#     ai_condition = condition if ai_condition is None else ai_condition | condition
# df = df.withColumn("IS_AI_ROLE", when(ai_condition, 1).otherwise(0))

# # add class weights for imbalance
# num_total = df.count()
# num_ai = df.filter(col("IS_AI_ROLE") == 1).count()
# num_non_ai = num_total - num_ai
# weight_ai = num_total / (2 * num_ai)
# weight_non_ai = num_total / (2 * num_non_ai)
# df = df.withColumn("classWeightCol", when(col("IS_AI_ROLE") == 1, weight_ai).otherwise(weight_non_ai))

# # encode categorical column
# naics_indexer = StringIndexer(inputCol="NAICS_2022_6", outputCol="NAICS_ENCODED", handleInvalid="keep")

# # assemble features
# assembler = VectorAssembler(inputCols=["NAICS_ENCODED", "AVG_YEARS_EXPERIENCE"], outputCol="features")

# # logistic regression with class weights
# lr = LogisticRegression(featuresCol="features", labelCol="IS_AI_ROLE", weightCol="classWeightCol", maxIter=20)

# # build pipeline
# pipeline = Pipeline(stages=[naics_indexer, assembler, lr])

# # split data into train and test
# train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)

# # fit model
# model = pipeline.fit(train_df)

# # make predictions
# predictions = model.transform(test_df)
# predictions.select("IS_AI_ROLE", "prediction", "probability").show(5)

# # evaluate roc auc
# roc_evaluator = BinaryClassificationEvaluator(labelCol="IS_AI_ROLE", rawPredictionCol="rawPrediction", metricName="areaUnderROC")
# roc_auc = roc_evaluator.evaluate(predictions)
# print(f"roc auc: {roc_auc}")

# # evaluate accuracy
# accuracy = predictions.filter(predictions.IS_AI_ROLE == predictions.prediction).count() / predictions.count()
# print(f"accuracy: {accuracy}")

# # evaluate precision, recall, f1
# tp = predictions.filter((col("IS_AI_ROLE") == 1) & (col("prediction") == 1)).count()
# fp = predictions.filter((col("IS_AI_ROLE") == 0) & (col("prediction") == 1)).count()
# fn = predictions.filter((col("IS_AI_ROLE") == 1) & (col("prediction") == 0)).count()
# precision = tp / (tp + fp) if tp + fp > 0 else 0
# recall = tp / (tp + fn) if tp + fn > 0 else 0
# f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0
# print(f"precision: {precision}, recall: {recall}, f1 score: {f1}")
```


```{python echo=false}
# from pyspark.ml.classification import DecisionTreeClassifier
# from pyspark.ml.evaluation import BinaryClassificationEvaluator

# # Build a decision tree classifier
# dt = DecisionTreeClassifier(featuresCol="features", labelCol="IS_AI_ROLE", maxDepth=5, maxBins=500)

# # Pipeline with same preprocessing
# dt_pipeline = Pipeline(stages=[naics_indexer, assembler, dt])

# # Fit the decision tree
# dt_model = dt_pipeline.fit(train_df)

# # Make predictions
# dt_predictions = dt_model.transform(test_df)

# # Evaluate
# roc_evaluator = BinaryClassificationEvaluator(labelCol="IS_AI_ROLE", rawPredictionCol="rawPrediction", metricName="areaUnderROC")
# dt_roc_auc = roc_evaluator.evaluate(dt_predictions)
# dt_accuracy = dt_predictions.filter(dt_predictions.IS_AI_ROLE == dt_predictions.prediction).count() / dt_predictions.count()

# print(f"Decision Tree ROC AUC: {dt_roc_auc}")
# print(f"Decision Tree Accuracy: {dt_accuracy}")

# # Optional: visualize the tree structure (text form)
# print(dt_model.stages[-1].toDebugString)
```


```{python echo=false}
# import matplotlib.pyplot as plt
# from pyspark.ml.classification import DecisionTreeClassifier
# from pyspark.ml.feature import StringIndexer, VectorAssembler
# from pyspark.ml import Pipeline
# from pyspark.sql.functions import substring, col
# from sklearn.tree import DecisionTreeClassifier as SkDecisionTree
# from sklearn import tree
# import pandas as pd

# df = df.withColumn("NAICS_MAJOR", substring(col("NAICS_2022_6").cast("string"), 1, 2))

# train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)

#decision tree pipeline
# naics_indexer = StringIndexer(inputCol="NAICS_MAJOR", outputCol="NAICS_ENCODED", handleInvalid="keep")
# assembler = VectorAssembler(inputCols=["NAICS_ENCODED", "AVG_YEARS_EXPERIENCE"], outputCol="features")

# dt = DecisionTreeClassifier(
#     featuresCol="features",
#     labelCol="IS_AI_ROLE",
#     maxDepth=5,
#     maxBins=1000
# )

# dt_pipeline = Pipeline(stages=[naics_indexer, assembler, dt])
# dt_model = dt_pipeline.fit(train_df)

#pandas visualization
# sample_pd = train_df.select("NAICS_MAJOR", "AVG_YEARS_EXPERIENCE", "IS_AI_ROLE").limit(10000).toPandas()
# sample_pd["NAICS_MAJOR"] = pd.factorize(sample_pd["NAICS_MAJOR"])[0]

# X = sample_pd[["NAICS_MAJOR", "AVG_YEARS_EXPERIENCE"]]
# y = sample_pd["IS_AI_ROLE"]

#trainin visualization
# sk_tree = SkDecisionTree(max_depth=5, random_state=42)
# sk_tree.fit(X, y)

#plot tree
# plt.figure(figsize=(18, 8))
# tree.plot_tree(
#     sk_tree,
#     feature_names=["NAICS_MAJOR", "AVG_YEARS_EXPERIENCE"],
#     class_names=["Non-AI", "AI"],
#     filled=True,
#     rounded=True,
#     fontsize=10
# )
# plt.title("Decision Tree Visualization (Sampled from Spark Model)", fontsize=14)
# plt.show()
```


# Logistic Regression Analysis

| Metric     | Value                    |
|------------|--------------------------|
| ROC AUC    | 0.6200                   |
| Accuracy   | 0.6384                   |
| Precision  | 0.0290                   |
| Recall     | 0.5152                   |
| F1 Score   | 0.0549                   |

We assessed the relationship between job characteristics and the likelihood of a role being AI-related using logistic regression. The model incorporated variables such as industry classification (NAICS codes), job titles, and experience levels, and we applied weights to account for the class imbalance, as AI-related postings represent a small minority of all jobs. From the analysis, we deduced that industries like information, software, and professional and technical services had a higher probability of including AI-related roles. Experience level contributed modestly, indicating that AI positions tend to require more specialized or senior expertise. The model achieved an ROC AUC of approximately 0.62 and an accuracy of 0.63, suggesting that while the logistic regression captured some patterns distinguishing AI and non-AI postings, there remains substantial overlap across industries and experience levels.

# Decision Tree Classifier

| Metric     | Value                    |
|------------|--------------------------|
| ROC AUC    | 0.4832                   |
| Accuracy   | 0.9765                   |

We then developed a decision tree classifier to better understand the factors that differentiate AI-related roles from non-AI ones. The tree achieved a high accuracy of about 97%, but its ROC AUC of 0.48 revealed that it primarily predicted the majority (non-AI) class correctly and struggled to identify minority AI postings. From the splits, we deduced that industry classification was the strongest predictor, consistently appearing near the root, while experience acted as a secondary factor. This suggests that whether a job is AI-related depends first on the industry and then on the required level of expertise. The results also highlighted the strong imbalance in AI postings, with only a few sectors consistently containing these roles.


# Decision Tree Visualization

![Decision Tree Visualization](figures/decisiontree.png)


Finally, we visualized the tree using Matplotlib to interpret the model’s decision logic. The first split on industry demonstrated that most sectors contained predominantly non-AI jobs. Subsequent splits based on years of experience showed that AI-related postings cluster mainly in technology- or data-driven sectors, particularly for positions requiring moderate-to-high experience. Most terminal nodes predicted non-AI roles, reinforcing that AI positions are still a niche subset of the job market. Overall, the visualization confirmed our earlier deduction: AI job prevalence is concentrated in a few industries, and while experience plays a supporting role, industry type is the dominant factor in identifying AI-focused roles.