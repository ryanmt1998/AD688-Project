---
title: "Your Title"
format:
    html:
        self-contained: true
jupyter: 
    kernel: python3
---


```{python}
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lower
from pyspark.sql.types import DoubleType
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
import pandas as pd
import hvplot.pandas
import panel as pn
pn.extension()
from itables import show
```


```{python}
## Load the Lightcast Job Posting Data
spark = SparkSession.builder \
    .appName("AI_vs_NonAI_JobPostings") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

df = spark.read.option("header", "true") \
        .option("inferSchema", "true") \
        .option("multiLine", "true") \
        .option("escape", "\"") \
        .csv("./data/lightcast_job_postings.csv")

```



```{python}
## Drop Columns
columns_to_drop = {'ID', 'URL', 'ACTIVE_URLS', 'DUPLICATES', 'LAST_UPDATED_TIMESTAMP',
    'NAICS2', 'NAICS3', 'NAICS4', 'NAICS5', 'NAICS6',
    'SOC_2', 'SOC_3', 'SOC_5'
}

df_cleaned = df.drop(*columns_to_drop)
```


```{python}

## AI vs Non-AI Classification

df_cleaned = df_cleaned.withColumn(
    "IS_AI_ROLE",
    when(
        (lower(col("TITLE_NAME")).contains("artificial intelligence")) |
        (lower(col("TITLE_NAME")).contains(" ai ")) |
        (lower(col("TITLE_NAME")).contains("machine learning")) |
        (lower(col("TITLE_NAME")).contains("data scientist")) |
        (lower(col("TITLE_NAME")).contains("ml engineer")) |
        (lower(col("SKILLS_NAME")).contains("machine learning")) |
        (lower(col("SKILLS_NAME")).contains("artificial intelligence")) |
        (lower(col("SKILLS_NAME")).contains("deep learning")) |
        (lower(col("SKILLS_NAME")).contains("neural network")),
        1
    ).otherwise(0)
)


```

```{python}
##Data Cleaning

df_clean = df_cleaned.dropna(subset=["MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "SALARY_FROM", "SALARY_TO", "NAICS_2022_6_NAME", "TITLE_NAME"])






```

```{python}
##AVERAGE SALARY

df_clean = df_clean.withColumn("AVG_SALARY", ((col("SALARY_FROM") + col("SALARY_TO")) / 2).cast(DoubleType())
)



```

```{python}
##Relevant Columns

df_casted = df_clean.select(
        col("TITLE_NAME"),
        col("NAICS_2022_6_NAME"),
        col("MIN_YEARS_EXPERIENCE").cast(DoubleType()),
        col("MAX_YEARS_EXPERIENCE").cast(DoubleType()),
        col("AVG_SALARY"),
        col("IS_AI_ROLE").cast(DoubleType())
).dropna()

show(df_casted.toPandas().head())
```

```{python}
## StringIndexers
title_indexer = StringIndexer(inputCol="TITLE_NAME", outputCol="TITLE_IDX", handleInvalid="skip")
industry_indexer = StringIndexer(inputCol="NAICS_2022_6_NAME", outputCol="INDUSTRY_IDX", handleInvalid="skip")



```

```{python}
##Vector Assembler
assembler = VectorAssembler(
    inputCols=["MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "AVG_SALARY", "TITLE_IDX", "INDUSTRY_IDX"],
    outputCol="features"
)
```

```{python}
##scaler
from pyspark.ml.feature import StandardScaler
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withMean=True, withStd=True)



```

```{python}
##K-means Clustering Model
kmeans = KMeans(featuresCol="scaledFeatures", predictionCol="cluster", k=3, seed=1)



```

```{python}
##Pipeline
pipeline = Pipeline(stages = [title_indexer, industry_indexer, assembler, scaler, kmeans])

model = pipeline.fit(df_casted)

```

```{python}
df_clustered = model.transform(df_casted)
evaluator = ClusteringEvaluator(featuresCol="scaledFeatures", predictionCol="cluster")
silhouette = evaluator.evaluate(df_clustered)
```

```{python}
##Cluster Count
from pyspark.sql.functions import avg
#Size of each cluster
df_clustered.groupBy("cluster").count().show()

##The distribution of AI vs non-AI roles within each cluster.
df_clustered.groupBy("cluster", "IS_AI_ROLE").count().orderBy("cluster", "IS_AI_ROLE").show()

##Average salaries between AI and non-AI roles
df_clustered.groupBy("IS_AI_ROLE").agg(
    avg("AVG_SALARY").alias("avg_salary")
).show()

##Most common industries for AI and non-AI roles
df_clustered.groupBy("IS_AI_ROLE", "NAICS_2022_6_NAME").count().orderBy("IS_AI_ROLE", "count", ascending=False).show()

##Most frequent job titles in AI vs non-AI roles
df_clustered.groupBy("IS_AI_ROLE", "TITLE_NAME").count().orderBy("IS_AI_ROLE", "count", ascending=False).show()



```

```{python}
# Interactive EDA: Pairwise Plots with HoloViz
import pandas as pd
import hvplot.pandas
import panel as pn
pn.extension()

# Use cleaned PDF
pdf = df_clustered.select(
    "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "AVG_SALARY", "IS_AI_ROLE"
).toPandas()

cols = ["MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "AVG_SALARY"]
pdf[cols] = pdf[cols].apply(pd.to_numeric, errors='coerce')
pdf = pdf.dropna(subset=cols + ["IS_AI_ROLE"])

plots = []
for x in cols:
    for y in cols:
        if x != y:
            plot = pdf.hvplot.scatter(
                x=x, y=y, by='IS_AI_ROLE', width=450, height=250, alpha=0.6, title=f"{y} vs {x}"
            )            plots.append(plot)

pn.GridBox(*plots, ncols=2)





```

```{python}



```
