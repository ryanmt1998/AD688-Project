---
title: "Your Title"
format:
    html:
        self-contained: true
jupyter: 
    kernel: python3
---






## K-Mean Clustering

```{python}
from pyspark.sql import SparkSession
import pandas as pd
from pyspark.sql import functions as F
from pyspark.sql.types import StringType
from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml import Pipeline
import matplotlib.pyplot as plt
import seaborn as sns
```

```{python}
# Initialize Spark Session
spark = SparkSession.builder.appName("./data/LightcastData").getOrCreate()

df = spark.read.option("header", "true") \
            .option("inferSchema", "true") \
            .option("multiLine", "true") \
            .option("escape", "\"") \
            .csv("./data/lightcast_job_postings.csv")
```

```{python}
def label_ai_roles(title):
    ai_keywords = ["AI", "artificial intelligence", "machine learning", 
                "data scientist", "deep learning"]
    title_lower = str(title).lower() if title else ''
    return "AI" if any(keyword in title_lower for keyword in ai_keywords) else "Non-AI"


label_ai_udf = F.udf(label_ai_roles, StringType())

df = df.withColumn('ai_label', label_ai_udf("TITLE_RAW"))
```

```{python}
df.printSchema()
```

```{python}

df = df.select("TITLE_RAW", "SALARY", "LIGHTCAST_SECTORS", "NAICS_2022_6", "ai_label")


```

```{python}
indexers = [
    StringIndexer(inputCol="TITLE_RAW", outputCol="job_title_enc", handleInvalid="keep"),
    StringIndexer(inputCol="LIGHTCAST_SECTORS", outputCol="industry_enc", handleInvalid="keep"),
    StringIndexer(inputCol="NAICS_2022_6", outputCol="naics_enc", handleInvalid="keep"),
    StringIndexer(inputCol="ai_label", outputCol="ai_enc", handleInvalid="keep")
]

pipeline = Pipeline(stages=indexers)
df_indexed = pipeline.fit(df).transform(df)

print("Categorical variables encoded to numbers")

```



```{python}

df_indexed_clean = df_indexed.na.drop(subset=["job_title_enc", "SALARY", "industry_enc", "naics_enc"])

```


```{python}
assembler = VectorAssembler(
    inputCols=["job_title_enc", "industry_enc", "SALARY", "naics_enc"],
    outputCol="features"
)
df_features = assembler.transform(df_indexed_clean)
```

```{python}
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
scaler_model = scaler.fit(df_features)
df_scaled = scaler_model.transform(df_features)

```

```{python}
from pyspark.ml.clustering import KMeans as SparkKMeans
from sklearn.cluster import KMeans as SklearnKMeans
import numpy as np
import matplotlib.pyplot as plt

# Convert scaled features column to numpy array for scikit-learn KMeans
X_scaled = np.array(df_scaled.select("scaled_features").toPandas()["scaled_features"].tolist())

# Elbow method using scikit-learn KMeans
inertias = []
for k in range(2, 11):
    kmeans = SklearnKMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

plt.figure(figsize=(8,5))
plt.plot(range(2, 11), inertias, "bo-", linewidth=2)
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia")
plt.title("Elbow Method for Optimal K")
plt.grid(True)
plt.show()
```

```{python}
kmeans = SparkKMeans(featuresCol="scaled_features", predictionCol="cluster", k=3, seed=42)
model = kmeans.fit(df_scaled)
df_clustered = model.transform(df_scaled)


```

```{python}
df_clean = df_clustered.select(
    "cluster", 
    "SALARY", 
    "ai_label", 
    "TITLE_RAW", 
    "LIGHTCAST_SECTORS",
    "job_title_enc",
    "industry_enc",
    "naics_enc"
)

```



```{python}
##Salary Comparison
print("\n" + "="*80)
print("1. SALARY COMPARISON")
print("="*80)

salary_comparison = df_clean.groupBy("ai_label").agg(
    F.mean("SALARY").alias("mean"),
    F.expr("percentile_approx(SALARY, 0.5)").alias("median"),
    F.stddev("SALARY").alias("std"),
    F.count("SALARY").alias("count")
)

salary_comparison.show()


ai_salary = salary_comparison.filter(F.col("ai_label") == "AI").select("mean").collect()[0][0]
non_ai_salary = salary_comparison.filter(F.col("ai_label") == "Non-AI").select("mean").collect()[0][0]
salary_diff = ai_salary - non_ai_salary

print("\nAI roles pay $" + str(round(salary_diff, 2)) + " more on average")
```

```{python}
##Job Title Comparison
print("\n" + "="*80)
print("2. TOP JOB TITLES")
print("="*80)

print("\nTop 10 Non-AI Job Titles:")
(df_clean.filter(F.col("ai_label") == "Non-AI")
    .groupBy("TITLE_RAW")
    .count()
    .orderBy(F.desc("count"))
    .show(10, truncate=False))

print("\nTop 10 AI Job Titles:")
(df_clean.filter(F.col("ai_label") == "AI")
    .groupBy("TITLE_RAW")
    .count()
    .orderBy(F.desc("count"))
    .show(10, truncate=False))
```

```{python}
##Cluster Distribution
print("\n" + "="*80)
print("3. CLUSTER DISTRIBUTION BY AI LABEL")
print("="*80)

cluster_dist = (df_clean.groupBy("cluster", "ai_label")
        .count()
        .orderBy("cluster", "ai_label")
)
cluster_dist.show()



```

```{python}
##Industry Comparison
print("\n" + "="*80)
print("4. Industry Comparison")
print("="*80)

print("\nTop 10 Non-AI Industries:")
(df_clean.filter(F.col("ai_label") == "Non-AI")
        .groupBy("LIGHTCAST_SECTORS")
        .count()
        .orderBy(F.desc("count"))
        .show(10, truncate = False)
)

print("\nTop 10 AI Industries:")
(df_clean.filter(F.col("ai_label") == "AI")
        .groupBy("LIGHTCAST_SECTORS")
        .count()
        .orderBy(F.desc("count"))
        .show(10, truncate = False)
)
```

```{python}
##Salary by Industry
print("\n" + "="*80)
print("5. AVERAGE SALARY BY INDUSTRY (Top 10)")
print("="*80)

print("\nNon-AI Roles - Top 10 Industries by Average Salary:")
(df_clean.filter(F.col("ai_label")=="Non-AI")
        .groupBy("LIGHTCAST_SECTORS")
        .agg(F.mean("SALARY").alias("avg_salary"), F.count("SALARY").alias("job_count") )
        .filter(F.col("job_count")>=10)
        .orderBy(F.desc("avg_salary"))
        .show(10,truncate=False)   
)

print("\nAI Roles - Top 10 Industries by Average Salary:")
(df_clean.filter(F.col("ai_label")=="AI")
        .groupBy("LIGHTCAST_SECTORS")
        .agg(F.mean("SALARY").alias("avg_salary"), F.count("SALARY").alias("job_count") )
        .filter(F.col("job_count")>=10)
        .orderBy(F.desc("avg_salary"))
        .show(10,truncate=False)   
)


```



```{python}
pdf = df_clean.sample(fraction=0.1, seed=42).toPandas()
```

```{python}
# Interactive EDA: Pairwise Plots with HoloViz
cols = ["SALARY", "job_title_enc", "industry_enc", "naics_enc"]
pdf[cols] = pdf[cols].apply(pd.to_numeric, errors='coerce')
pdf = pdf.dropna(subset=cols + ["ai_label"])

import hvplot.pandas
import panel as pn
pn.extension()

# Create scatter plots for all pairwise combinations
plots = []
for x in cols:
    for y in cols:
        if x != y:
            plot = pdf.hvplot.scatter(
                x=x, 
                y=y, 
                by='ai_label',  # Color by AI vs Non-AI
                width=450, 
                height=250, 
                alpha=0.6, 
                title=f"{y} vs {x}"
            )
            plots.append(plot)

# Display plots in a grid layout with 2 columns
pn.panel(pn.GridBox(*plots, ncols=2)).show()


```

```{python}




```

```{python}



```



## Random Forest Regression
```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

# Load dataset
df = pd.read_csv("data/lightcast_job_postings.csv", low_memory=False)

# Load dataset
df = pd.read_csv("data/lightcast_job_postings.csv", low_memory=False)

# Select only numeric/categorical features for salary prediction
features = [
    "MIN_YEARS_EXPERIENCE",
    "MAX_YEARS_EXPERIENCE",
    "NAICS_2022_6",
    "MIN_EDULEVELS",
    "MAX_EDULEVELS"
]

# Convert features to numeric, coerce errors to NaN
X = df[features].apply(pd.to_numeric, errors='coerce')

# Drop or fill missing values
X = X.fillna(0)

# Target variable
y = pd.to_numeric(df["SALARY"], errors='coerce').fillna(0)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Initialize and train Random Forest
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate model
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

print("R² Score:", r2_score(y_test, y_pred))

# Compute RMSE manually
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE:", rmse)


# Evaluate model
print("R² Score:", r2_score(y_test, y_pred))
print("RMSE:", mean_squared_error(y_test, y_pred, squared=False))

# Optional: feature importance
importances = pd.DataFrame({
    "Feature": X.columns,
    "Importance": model.feature_importances_
}).sort_values(by="Importance", ascending=False)

print("\nFeature Importances:")
print(importances)


```