---
title: "Your Title"
format:
    html:
        self-contained: true
jupyter: 
    kernel: python3
---


```{python}
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lower
from pyspark.sql.types import DoubleType
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
import pandas as pd
import hvplot.pandas
import panel as pn
pn.extension()
from itables import show
```


```{python}
## Load the Lightcast Job Posting Data
spark = SparkSession.builder \
    .appName("AI_vs_NonAI_JobPostings") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

df = spark.read.option("header", "true") \
        .option("inferSchema", "true") \
        .option("multiLine", "true") \
        .option("escape", "\"") \
        .csv("./data/lightcast_job_postings.csv")

```



```{python}
## Drop Columns
columns_to_drop = {'ID', 'URL', 'ACTIVE_URLS', 'DUPLICATES', 'LAST_UPDATED_TIMESTAMP',
    'NAICS2', 'NAICS3', 'NAICS4', 'NAICS5', 'NAICS6',
    'SOC_2', 'SOC_3', 'SOC_5'
}

df_cleaned = df.drop(*columns_to_drop)
```


```{python}

## AI vs Non-AI Classification

df_cleaned = df_cleaned.withColumn(
    "IS_AI_ROLE",
    when(
        (lower(col("TITLE_NAME")).contains("artificial intelligence")) |
        (lower(col("TITLE_NAME")).contains(" ai ")) |
        (lower(col("TITLE_NAME")).contains("machine learning")) |
        (lower(col("TITLE_NAME")).contains("data scientist")) |
        (lower(col("TITLE_NAME")).contains("ml engineer")) |
        (lower(col("SKILLS_NAME")).contains("machine learning")) |
        (lower(col("SKILLS_NAME")).contains("artificial intelligence")) |
        (lower(col("SKILLS_NAME")).contains("deep learning")) |
        (lower(col("SKILLS_NAME")).contains("neural network")),
        1
    ).otherwise(0)
)


```

```{python}
##Data Cleaning

df_clean = df_cleaned.dropna(subset=["MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "SALARY_FROM", "SALARY_TO", "NAICS_2022_6_NAME", "TITLE_NAME"])






```

```{python}
##AVERAGE SALARY

df_clean = df_clean.withColumn("AVG_SALARY", ((col("SALARY_FROM") + col("SALARY_TO")) / 2).cast(DoubleType())
)



```

```{python}
##Relevant Columns

df_casted = df_clean.select(
        col("TITLE_NAME"),
        col("NAICS_2022_6_NAME"),
        col("MIN_YEARS_EXPERIENCE").cast(DoubleType()),
        col("MAX_YEARS_EXPERIENCE").cast(DoubleType()),
        col("AVG_SALARY"),
        col("IS_AI_ROLE").cast(DoubleType())
).dropna()

show(df_casted.toPandas().head())
```

```{python}
## StringIndexers
title_indexer = StringIndexer(inputCol="TITLE_NAME", outputCol="TITLE_IDX", handleInvalid="skip")
industry_indexer = StringIndexer(inputCol="NAICS_2022_6_NAME", outputCol="INDUSTRY_IDX", handleInvalid="skip")



```

```{python}
##Vector Assembler
assembler = VectorAssembler(
    inputCols=["MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "AVG_SALARY", "TITLE_IDX", "INDUSTRY_IDX"],
    outputCol="features"
)
```

```{python}
##scaler
from pyspark.ml.feature import StandardScaler
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withMean=True, withStd=True)



```

```{python}
##K-means Clustering Model
kmeans = KMeans(featuresCol="scaledFeatures", predictionCol="cluster", k=3, seed=1)



```

```{python}
##Pipeline
pipeline = Pipeline(stages = [title_indexer, industry_indexer, assembler, scaler, kmeans])

model = pipeline.fit(df_casted)

```

```{python}
df_clustered = model.transform(df_casted)
evaluator = ClusteringEvaluator(featuresCol="scaledFeatures", predictionCol="cluster")
silhouette = evaluator.evaluate(df_clustered)
```

```{python}
##Cluster Count
from pyspark.sql.functions import avg
#Size of each cluster
df_clustered.groupBy("cluster").count().show()

##The distribution of AI vs non-AI roles within each cluster.
df_clustered.groupBy("cluster", "IS_AI_ROLE").count().orderBy("cluster", "IS_AI_ROLE").show()

##Average salaries between AI and non-AI roles
df_clustered.groupBy("IS_AI_ROLE").agg(
    avg("AVG_SALARY").alias("avg_salary")
).show()

##Most common industries for AI and non-AI roles
df_clustered.groupBy("IS_AI_ROLE", "NAICS_2022_6_NAME").count().orderBy("IS_AI_ROLE", "count", ascending=False).show()

##Most frequent job titles in AI vs non-AI roles
df_clustered.groupBy("IS_AI_ROLE", "TITLE_NAME").count().orderBy("IS_AI_ROLE", "count", ascending=False).show()



```

```{python}
# Interactive EDA: Pairwise Plots with HoloViz
import pandas as pd
import hvplot.pandas
import panel as pn
pn.extension()

# Use cleaned PDF
pdf = df_clustered.select(
    "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "AVG_SALARY", "IS_AI_ROLE"
).toPandas()

cols = ["MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "AVG_SALARY"]
pdf[cols] = pdf[cols].apply(pd.to_numeric, errors='coerce')
pdf = pdf.dropna(subset=cols + ["IS_AI_ROLE"])

plots = []
for x in cols:
    for y in cols:
        if x != y:
            plot = pdf.hvplot.scatter(
                x=x, y=y, by='IS_AI_ROLE', width=450, height=250, alpha=0.6, title=f"{y} vs {x}"
            )
            plots.append(plot)

pn.GridBox(*plots, ncols=2)






```

```{python}
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
import matplotlib.pyplot as plt
import seaborn as sns

ks = list(range(2, 11))
scores = []

for k in ks:
    kmeans = KMeans(featuresCol="scaledFeatures", predictionCol="cluster", k=k, seed=1)
    pipeline = Pipeline(stages=[title_indexer, industry_indexer, assembler, scaler, kmeans])
    model = pipeline.fit(df_casted)
    df_clustered = model.transform(df_casted)
    evaluator = ClusteringEvaluator(featuresCol="scaledFeatures", predictionCol="cluster")
    silhouette = evaluator.evaluate(df_clustered)
    scores.append(silhouette)
    print(f"k={k}, Silhouette Score={silhouette}")

# Plot silhouette scores vs k
sns.lineplot(x=ks, y=scores, marker="o")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")
plt.title("Silhouette Score vs. k (Elbow Method)")
plt.grid(True)
plt.show()





```

## Random Forest Regression

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

# Load dataset
df = pd.read_csv("lightcast_job_postings.csv", low_memory=False)

# Select only numeric/categorical features for salary prediction
features = [
    "MIN_YEARS_EXPERIENCE",
    "MAX_YEARS_EXPERIENCE",
    "NAICS_2022_6",
    "MIN_EDULEVELS",
    "MAX_EDULEVELS"
]

# Convert features to numeric, coerce errors to NaN
X = df[features].apply(pd.to_numeric, errors='coerce')

# Drop or fill missing values
X = X.fillna(0)

# Target variable
y = pd.to_numeric(df["SALARY"], errors='coerce').fillna(0)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Initialize and train Random Forest
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate model
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("R² Score:", r2)
print("RMSE:", rmse)

# Feature importance
importances = pd.DataFrame({
    "Feature": X.columns,
    "Importance": model.feature_importances_
}).sort_values(by="Importance", ascending=False)

print("\nFeature Importances:")
print(importances)


```
## Findings (will write paragraph next)
R² Score: 0.3260268288195246
RMSE: 53928.331046621715

Feature Importances:
                Feature  Importance
2          NAICS_2022_6    0.534498
0  MIN_YEARS_EXPERIENCE    0.259279
3         MIN_EDULEVELS    0.093647
4         MAX_EDULEVELS    0.057796
1  MAX_YEARS_EXPERIENCE    0.054780





