## Cluster
```{python}
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lower
from pyspark.sql.types import DoubleType
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
import pandas as pd
import hvplot.pandas
import panel as pn
pn.extension()
from itables import show
```




```{python}
## Load the Lightcast Job Posting Data
spark = SparkSession.builder \
.appName("AI_vs_NonAI_JobPostings") \
.config("spark.driver.memory", "4g") \
.getOrCreate()


df = spark.read.option("header", "true") \
    .option("inferSchema", "true") \
    .option("multiLine", "true") \
    .option("escape", "\"") \
    .csv("./data/lightcast_job_postings.csv")

```


```{python}
## Drop Columns
columns_to_drop = {'ID', 'URL', 'ACTIVE_URLS', 'DUPLICATES', 'LAST_UPDATED_TIMESTAMP',
'NAICS2', 'NAICS3', 'NAICS4', 'NAICS5', 'NAICS6',
'SOC_2', 'SOC_3', 'SOC_5'
}


df_cleaned = df.drop(*columns_to_drop)
```


```{python}

## AI vs Non-AI Classification

df_cleaned = df_cleaned.withColumn(
"IS_AI_ROLE",
when(
    (lower(col("TITLE_NAME")).contains("artificial intelligence")) |
    (lower(col("TITLE_NAME")).contains(" ai ")) |
    (lower(col("TITLE_NAME")).contains("machine learning")) |
    (lower(col("TITLE_NAME")).contains("data scientist")) |
    (lower(col("TITLE_NAME")).contains("ml engineer")) |
    (lower(col("SKILLS_NAME")).contains("machine learning")) |
    (lower(col("SKILLS_NAME")).contains("artificial intelligence")) |
    (lower(col("SKILLS_NAME")).contains("deep learning")) |
    (lower(col("SKILLS_NAME")).contains("neural network")),
    1
).otherwise(0)
)


```


```{python}
##Data Cleaning


df_clean = df_cleaned.dropna(subset=["MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "SALARY_FROM", "SALARY_TO", "NAICS_2022_6_NAME", "TITLE_NAME"])


```


```{python}
##AVERAGE SALARY

df_clean = df_clean.withColumn("AVG_SALARY", ((col("SALARY_FROM") + col("SALARY_TO")) / 2).cast(DoubleType())
)

```


```{python}
##Relevant Columns

df_casted = df_clean.select(
    col("TITLE_NAME"),
    col("NAICS_2022_6_NAME"),
    col("MIN_YEARS_EXPERIENCE").cast(DoubleType()),
    col("MAX_YEARS_EXPERIENCE").cast(DoubleType()),
    col("AVG_SALARY"),
    col("IS_AI_ROLE").cast(DoubleType())
).dropna()


show(df_casted.toPandas().head())
```


```{python}
## StringIndexers
title_indexer = StringIndexer(inputCol="TITLE_NAME", outputCol="TITLE_IDX", handleInvalid="skip")
industry_indexer = StringIndexer(inputCol="NAICS_2022_6_NAME", outputCol="INDUSTRY_IDX", handleInvalid="skip")

```


```{python}
##Vector Assembler
assembler = VectorAssembler(
inputCols=["MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "AVG_SALARY", "TITLE_IDX", "INDUSTRY_IDX"],
outputCol="features"
)
```


```{python}
##K-means Clustering Model
kmeans = KMeans(featuresCol="features", predictionCol="cluster", k=3, seed=1)
```


```{python}
##Pipeline
pipeline = Pipeline(stages = [title_indexer, industry_indexer, assembler, kmeans])

model = pipeline.fit(df_casted)

```


```{python}
df_clustered = model.transform(df_casted)
evaluator = ClusteringEvaluator(featuresCol="features", predictionCol="cluster")
silhouette = evaluator.evaluate(df_clustered)
```


```{python}
from pyspark.sql.functions import avg
##Cluster Count
df_clustered.groupBy("cluster").count().show()

##The distribution of AI vs non-AI roles within each cluster.
df_clustered.groupBy("cluster", "IS_AI_ROLE").count().orderBy("cluster", "IS_AI_ROLE").show()

##Average salaries between AI and non-AI roles
df_clustered.groupBy("IS_AI_ROLE").agg(
    avg("AVG_SALARY").alias("avg_salary")
).show()

##Most common industries for AI and non-AI roles
df_clustered.groupBy("IS_AI_ROLE", "NAICS_2022_6_NAME").count().orderBy("IS_AI_ROLE", "count", ascending=False).show()

##Most frequent job titles in AI vs non-AI roles
df_clustered.groupBy("IS_AI_ROLE", "TITLE_NAME").count().orderBy("IS_AI_ROLE", "count", ascending=False).show()
```



```{python}
# Interactive EDA: Pairwise Plots with HoloViz
import pandas as pd
import hvplot.pandas
import panel as pn
pn.extension()


# Use cleaned PDF
pdf = df_clustered.select(
"MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "AVG_SALARY", "IS_AI_ROLE"
).toPandas()


cols = ["MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "AVG_SALARY"]
pdf[cols] = pdf[cols].apply(pd.to_numeric, errors='coerce')
pdf = pdf.dropna(subset=cols + ["IS_AI_ROLE"])

plots = []
for x in cols:
    for y in cols:
        if x != y:
            plot = pdf.hvplot.scatter(
                x=x, y=y, by='IS_AI_ROLE', width=450, height=250, alpha=0.6, title=f"{y} vs {x}"
            )
            plots.append(plot)

pn.GridBox(*plots, ncols=2)
```






## Random Forest Classification part 1 - selecting ai vs non ai and observation
```{python}
import pandas as pd

df = pd.read_csv("lightcast_job_postings.csv", low_memory=False)

#TITLE_CLEAN is a string
df["TITLE_CLEAN"] = df["TITLE_CLEAN"].astype(str)

#defining AI-related keywords
ai_keywords = [
    "AI", "Artificial Intelligence", "Machine Learning", "Deep Learning",
    "Neural", "Data Scientist", "Computer Vision", "NLP", "Natural Language",
    "LLM", "Chatbot", "Generative", "Data Engineer", "Software", "Data"
]

#creatung AI_JOB column: 1 = AI job, 0 = Non-AI job
df["AI_JOB"] = df["TITLE_CLEAN"].apply(
    lambda x: 1 if any(keyword.lower() in x.lower() for keyword in ai_keywords) else 0
)

#shows how many AI vs Non-AI jobs
print(df["AI_JOB"].value_counts())

#
print(df.groupby("AI_JOB")["SALARY"].describe())

df.groupby("AI_JOB")["NAICS_2022_6_NAME"].value_counts().head(10)

```

In this dataset of 72,498 job postings, 30,197 positions (≈42%) were classified as AI-related, while 42,301 (≈58%) were non-AI. Non-AI roles have a slightly higher average salary ($128K vs. $105K for AI roles) and show substantial variation, with salaries ranging from $15K to $500K. Non-AI jobs are concentrated in consulting, administrative, and computer-related services, whereas AI roles are more dispersed across technical and data-focused industries. These patterns highlight differences in compensation and industry focus between AI and non-AI careers.



## Random Forest Classification part 2 - features used to determine to predict ai vs non ai job  
```{python}
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

features = ["SOFTWARE_SKILLS", "SPECIALIZED_SKILLS", "MIN_EDULEVELS", "MAX_EDULEVELS", "NAICS_2022_6_NAME", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "SOC_5_NAME" ]
X = df[features].apply(pd.to_numeric, errors='coerce').fillna(0)
y = df["AI_JOB"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train, y_train)
y_pred = rf_clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

```

Using a Random Forest classifier to predict AI-related jobs based on skills, education, experience, industry, and job title, the model achieved an overall accuracy of 63%. The classifier performs better at identifying non-AI jobs (recall 0.80) than AI jobs (recall 0.40), indicating it is more likely to misclassify AI roles. Precision and F1-scores reflect this imbalance, with non-AI roles having higher scores than AI roles. These results suggest that while features like education, experience, and industry provide some predictive power, distinguishing AI from non-AI positions remains challenging with the current feature set.

## Logistic Regression
```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Initialize and train Logistic Regression
log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg.fit(X_train, y_train)

# Make predictions
y_pred_log = log_reg.predict(X_test)

# Evaluate model
print("Accuracy:", accuracy_score(y_test, y_pred_log))
print(classification_report(y_test, y_pred_log))

```
The Logistic Regression Model has similar results to the RFC because the features are not too strong, leading to such outcomes.

